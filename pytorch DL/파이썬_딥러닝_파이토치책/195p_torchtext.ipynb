{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import data \n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(lower = True, batch_first = True)\n",
    "LABEL = data.Field(sequential=True)\n",
    "\n",
    "train, test = datasets.IMDB.splits(TEXT, LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Not found: \"spm.model\": No such file or directory Error #2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Admin\\TIL\\pytorch DL\\파이썬_딥러닝_파이토치책\\195p_torchtext.ipynb 셀 2\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/TIL/pytorch%20DL/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EC%B1%85/195p_torchtext.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msentencepiece\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspm\u001b[39;00m \n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Admin/TIL/pytorch%20DL/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EC%B1%85/195p_torchtext.ipynb#W2sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m s \u001b[39m=\u001b[39m spm\u001b[39m.\u001b[39;49mSentencePieceProcessor(model_file \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mspm.model\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/TIL/pytorch%20DL/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EC%B1%85/195p_torchtext.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m) :\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Admin/TIL/pytorch%20DL/%ED%8C%8C%EC%9D%B4%EC%8D%AC_%EB%94%A5%EB%9F%AC%EB%8B%9D_%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98%EC%B1%85/195p_torchtext.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     s\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mNew  York\u001b[39m\u001b[39m'\u001b[39m, out_type \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m, enable_sampling \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, alpha\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m, nbset\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sentencepiece\\__init__.py:447\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Init\u001b[1;34m(self, model_file, model_proto, out_type, add_bos, add_eos, reverse, emit_unk_piece, enable_sampling, nbest_size, alpha, num_threads)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_threads \u001b[39m=\u001b[39m num_threads\n\u001b[0;32m    446\u001b[0m \u001b[39mif\u001b[39;00m model_file \u001b[39mor\u001b[39;00m model_proto:\n\u001b[1;32m--> 447\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLoad(model_file\u001b[39m=\u001b[39;49mmodel_file, model_proto\u001b[39m=\u001b[39;49mmodel_proto)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sentencepiece\\__init__.py:905\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    903\u001b[0m \u001b[39mif\u001b[39;00m model_proto:\n\u001b[0;32m    904\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 905\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mLoadFromFile(model_file)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\sentencepiece\\__init__.py:310\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLoadFromFile\u001b[39m(\u001b[39mself\u001b[39m, arg):\n\u001b[1;32m--> 310\u001b[0m     \u001b[39mreturn\u001b[39;00m _sentencepiece\u001b[39m.\u001b[39;49mSentencePieceProcessor_LoadFromFile(\u001b[39mself\u001b[39;49m, arg)\n",
      "\u001b[1;31mOSError\u001b[0m: Not found: \"spm.model\": No such file or directory Error #2"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm \n",
    "s = spm.SentencePieceProcessor(model_file = 'spm.model')\n",
    "for n in range(5) :\n",
    "    s.encode('New  York', out_type = str, enable_sampling = True, alpha=0.1, nbset=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b458283c394a45b702cfa651be66c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84966132a6e46d5b67c771f2a897a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af48388c6f6f4d2e949b014b7b6a1f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'dog', 'is', 'cute', '.', 'he', 'likes', 'playing']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fafa265fd6427ca8d1a408ad135f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/872k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "647e12ad48664f47be25ff5a996f306c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102d0e3dd4294452bffec29ff32c30b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105879\n",
      "['my', 'dog', 'is', 'cut', '##e', '.', 'he', 'likes', 'playing']\n",
      "['나는', 'ᄎ', '##ᅢᆨ', '##상', '위에', 'ᄉ', '##ᅡ', '##과', '##를', 'ᄆ', '##ᅥ', '##ᆨ', '##었다', '.', '알', '##고', 'ᄇ', '##ᅩ', '##니', '그', 'ᄉ', '##ᅡ', '##과', '##는', 'jason', '것이', '##었다', '.', '그', '##래', '##서', 'jason', '##에게', 'ᄉ', '##ᅡ', '##과', '##를', '했다']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'My dog is cute. He likes playing'\n",
    "print(tokenizer.tokenize(sentence))\n",
    "\n",
    "# multilingual 은 다양한 언어 포함\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "print(len(tokenizer.vocab))\n",
    "print(tokenizer.tokenize(sentence))\n",
    "\n",
    "sentence  = '나는 책상 위에 사과를 먹었다. 알고 보니 그 사과는 Jason 것이었다. 그래서 Jason에게 사과를 했다'\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torchtext import data, datasets\n",
    "TEXT = data.Field(batch_first = True, \n",
    "                  fix_length = 500,\n",
    "                  tokenize = str.split,\n",
    "                  pad_first = True,\n",
    "                  pad_token = '[PAD]',\n",
    "                  unk_token='[UNK]')\n",
    "\n",
    "LABEL = data.LabelField(dtype = torch.float)\n",
    "\n",
    "train_data, test_data  = datasets.IMDB.splits(text_field = TEXT,\n",
    "                                             label_field = LABEL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
