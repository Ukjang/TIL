{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channels_last\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image \n",
    "import numpy as np \n",
    "import math \n",
    "import os \n",
    "\n",
    "import tensorflow as tf \n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras import models, layers, optimizers \n",
    "from keras.models import Model, Sequential \n",
    "from keras.layers import Input, Dense, Conv2D, BatchNormalization, \\\n",
    "                    Reshape, UpSampling2D, MaxPooling2D, Flatten, UpSampling3D, MaxPooling3D\n",
    "import keras.backend as K \n",
    "print(K.image_data_format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_4d(y_true, y_pred) :\n",
    "    return K.mean(K.square(y_pred - y_true), axis = (1, 2, 3))\n",
    "\n",
    "def mse_4d_tf(y_true, y_pred) :\n",
    "# tf.reduce_mean 은 열단위로 연산\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true), axis = (1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Sequential) :\n",
    "    def __init__(self, input_dim = 32):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim \n",
    "        \n",
    "        self.generator = self.GENERATOR()\n",
    "        self.discriminator = self.DISCRIMINATOR()\n",
    "        self.add(self.generator)\n",
    "        self.discriminator.trainable = False \n",
    "        self.add(self.discriminator)\n",
    "\n",
    "        self.compile_all() \n",
    "\n",
    "    def compile_all(self) :\n",
    "        d_optim = optimizers.SGD(lr = 5e-4, momentum=0.9, nesterov = True)\n",
    "        g_optim = optimizers.SGD(lr = 5e-4, momentum=0.9, nesterov = True)\n",
    "        self.generator.compile(loss = mse_4d_tf, optimizer='SGD')\n",
    "        self.compile(loss = 'binary_crossentropy', optimizer= g_optim)\n",
    "        self.discriminator.trainable = True \n",
    "        self.discriminator.compile(loss = 'binary_crossentropy', optimizer = d_optim)\n",
    "\n",
    "    def GENERATOR(self) :\n",
    "        input_dim = self.input_dim \n",
    "\n",
    "        model = Sequential([\n",
    "            Dense(1024, activation='relu', input_dim = input_dim),\n",
    "            Dense(8 * 8 * 128, activation='tanh'),\n",
    "            BatchNormalization(),\n",
    "            Reshape((8, 8, 128), input_shape = (8 * 8 * 128 , )),\n",
    "            UpSampling2D(size = (2, 2)),\n",
    "            Conv2D(128, (5,5), padding = 'same', activation='tanh'),\n",
    "            UpSampling2D(size = (2, 2)),\n",
    "            # UpSampling 두번을 거쳤기떄문에 이미지 크기 28 * 28\n",
    "            Conv2D(3, (5, 5), padding = 'same', activation='tanh')\n",
    "        ])\n",
    "        return model \n",
    "    \n",
    "    def DISCRIMINATOR(self) :\n",
    "        model = Sequential([\n",
    "            Conv2D(128, (5, 5), padding = 'same', activation= 'tanh',\n",
    "                   input_shape = (32, 32, 3)),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Conv2D(256, (5, 5), activation='tanh'),\n",
    "            MaxPooling2D(pool_size = (2, 2)),\n",
    "            Flatten(),\n",
    "            # MaxPooling 두번 겨쳐 처음 생성했던 7*7 크기로 돌아옴\n",
    "            Dense(1024, activation='tanh'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        return model \n",
    "    \n",
    "    def get_z(self, ln) :\n",
    "        input_dim = self.input_dim \n",
    "        # np.random.uniform(min, max, size)\n",
    "        return np.random.uniform(-1, 1, (ln, input_dim))\n",
    "        # -1 ~ 1로 스캐일링하여 노이지 생성\n",
    "        # 이후 *127.5 + 127.5로 0~255사이의 값으로 맞춰줌\n",
    "\n",
    "    def train_both(self, x) :\n",
    "        # discriminator 훈련\n",
    "        ln = x.shape[0]\n",
    "        # 노이즈 생성\n",
    "        z = self.get_z(ln)\n",
    "        # 노이즈 기반 생성값\n",
    "        w = self.generator.predict(z, verbose= 0)\n",
    "        # 실제 이미지 값 + 생성된 이미지값\n",
    "        xw = np.concatenate((x, w))\n",
    "        y2 = np.array([1] * ln + [0] * ln).reshape(-1, 1)\n",
    "        d_loss = self.discriminator.train_on_batch(xw, y2)\n",
    "\n",
    "        # 제너레이터 훈련\n",
    "        z = self.get_z(ln)\n",
    "        self.discriminator.trainable = False \n",
    "        # 가짜로 만든 이미지가 진짜로 인식하도록\n",
    "        g_loss = self.train_on_batch(z, np.array([1] * ln).reshape(-1, 1))\n",
    "        self.discriminator.trainable = True \n",
    "\n",
    "        return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_images(generated_images) :\n",
    "    num = generated_images.shape[0]\n",
    "    # 배치사이즈에 비례해서 정사각형 형태로 그려줌\n",
    "    width = int(math.sqrt(num))\n",
    "    height = int(math.ceil(float(num) / width ))\n",
    "    shape = generated_images.shape[1 : 4]\n",
    "    image = np.zeros((height * shape[0], width * shape[1], shape[2]),\n",
    "                     dtype = generated_images.dtype)\n",
    "    # 배치사이즈별로 칸 맞춰서 그려줌\n",
    "    for index, img in enumerate(generated_images) :\n",
    "        i = int(index / width)\n",
    "        j = index % width \n",
    "        image[i * shape[0]:(i+1)*shape[0],\n",
    "              j * shape[1]:(j+1)*shape[1], :] = img[:, :, :]\n",
    "    return image \n",
    "\n",
    "def get_x(X_train, index, BATCH_SIZE) :\n",
    "    return X_train[index *  BATCH_SIZE:(index + 1) * BATCH_SIZE]\n",
    "\n",
    "def save_images(generated_images, output_fold, epoch, index) :\n",
    "    image = combine_images(generated_images)\n",
    "    image = image * 127.5 + 127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\n",
    "        output_fold + '/' +\n",
    "        str(epoch) + '_' + str(index) + '.png'\n",
    "    )\n",
    "\n",
    "def load_data(n_train) :\n",
    "    (X_train, y_train), (_, _) = cifar10.load_data()\n",
    "    return X_train[:n_train]\n",
    "\n",
    "def train(args) :\n",
    "    BATCH_SIZE = args.batch_size \n",
    "    epochs = args.epochs \n",
    "    output_fold = args.output_fold \n",
    "    input_dim = args.input_dim \n",
    "    n_train = args.n_train \n",
    "    \n",
    "    os.makedirs(output_fold, exist_ok= True)\n",
    "    print('Outpug_fold is', output_fold)\n",
    "\n",
    "    X_train = load_data(n_train)\n",
    "\n",
    "    X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "    # X_train = X_train.reshape(X_train.shape + (3,)) \n",
    "\n",
    "\n",
    "    gan = GAN(input_dim)\n",
    "\n",
    "    d_loss_ll = []\n",
    "    g_loss_ll = []\n",
    "    for epoch in range(epochs) :\n",
    "        if epoch % 10 == 0 :\n",
    "            print('Epoch is', epoch)\n",
    "            print('Number of batches', int(X_train.shape[0] / BATCH_SIZE))\n",
    "        \n",
    "        d_loss_l = []\n",
    "        g_loss_l = []\n",
    "        for index in range(int(X_train.shape[0] / BATCH_SIZE)) :\n",
    "            x = get_x(X_train, index, BATCH_SIZE)\n",
    "\n",
    "            d_loss, g_loss = gan.train_both(x)\n",
    "\n",
    "            d_loss_l.append(d_loss) \n",
    "            g_loss_l.append(g_loss)\n",
    "\n",
    "        if epoch % 10 == 0 or epoch == epochs - 1 :\n",
    "            z = gan.get_z(x.shape[0])\n",
    "            w = gan.generator.predict(z, verbose = 0)\n",
    "            save_images(w, output_fold, epoch, 0)\n",
    "\n",
    "        d_loss_ll.append(d_loss_l)\n",
    "        g_loss_ll.append(g_loss_l)\n",
    "\n",
    "    gan.generator.save_weights(output_fold + '/' + 'generator', True)\n",
    "    gan.discriminator.save_weights(output_fold + '/' + 'discriminator', True)\n",
    "\n",
    "    np.savetxt(output_fold + '/' + 'd_loss', d_loss_ll)\n",
    "    np.savetxt(output_fold + '/' + 'g_loss', g_loss_ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outpug_fold is GAN_OUT_0605_cifar10\n",
      "Epoch is 0\n",
      "Number of batches 4\n",
      "Epoch is 10\n",
      "Number of batches 4\n",
      "Epoch is 20\n",
      "Number of batches 4\n",
      "Epoch is 30\n",
      "Number of batches 4\n",
      "Epoch is 40\n",
      "Number of batches 4\n",
      "Epoch is 50\n",
      "Number of batches 4\n",
      "Epoch is 60\n",
      "Number of batches 4\n",
      "Epoch is 70\n",
      "Number of batches 4\n",
      "Epoch is 80\n",
      "Number of batches 4\n",
      "Epoch is 90\n",
      "Number of batches 4\n",
      "Epoch is 100\n",
      "Number of batches 4\n",
      "Epoch is 110\n",
      "Number of batches 4\n",
      "Epoch is 120\n",
      "Number of batches 4\n",
      "Epoch is 130\n",
      "Number of batches 4\n",
      "Epoch is 140\n",
      "Number of batches 4\n",
      "Epoch is 150\n",
      "Number of batches 4\n",
      "Epoch is 160\n",
      "Number of batches 4\n",
      "Epoch is 170\n",
      "Number of batches 4\n",
      "Epoch is 180\n",
      "Number of batches 4\n",
      "Epoch is 190\n",
      "Number of batches 4\n",
      "Epoch is 200\n",
      "Number of batches 4\n",
      "Epoch is 210\n",
      "Number of batches 4\n",
      "Epoch is 220\n",
      "Number of batches 4\n",
      "Epoch is 230\n",
      "Number of batches 4\n",
      "Epoch is 240\n",
      "Number of batches 4\n",
      "Epoch is 250\n",
      "Number of batches 4\n",
      "Epoch is 260\n",
      "Number of batches 4\n",
      "Epoch is 270\n",
      "Number of batches 4\n",
      "Epoch is 280\n",
      "Number of batches 4\n",
      "Epoch is 290\n",
      "Number of batches 4\n",
      "Epoch is 300\n",
      "Number of batches 4\n",
      "Epoch is 310\n",
      "Number of batches 4\n",
      "Epoch is 320\n",
      "Number of batches 4\n",
      "Epoch is 330\n",
      "Number of batches 4\n",
      "Epoch is 340\n",
      "Number of batches 4\n",
      "Epoch is 350\n",
      "Number of batches 4\n",
      "Epoch is 360\n",
      "Number of batches 4\n",
      "Epoch is 370\n",
      "Number of batches 4\n",
      "Epoch is 380\n",
      "Number of batches 4\n",
      "Epoch is 390\n",
      "Number of batches 4\n",
      "Epoch is 400\n",
      "Number of batches 4\n",
      "Epoch is 410\n",
      "Number of batches 4\n",
      "Epoch is 420\n",
      "Number of batches 4\n",
      "Epoch is 430\n",
      "Number of batches 4\n",
      "Epoch is 440\n",
      "Number of batches 4\n",
      "Epoch is 450\n",
      "Number of batches 4\n",
      "Epoch is 460\n",
      "Number of batches 4\n",
      "Epoch is 470\n",
      "Number of batches 4\n",
      "Epoch is 480\n",
      "Number of batches 4\n",
      "Epoch is 490\n",
      "Number of batches 4\n",
      "Epoch is 500\n",
      "Number of batches 4\n",
      "Epoch is 510\n",
      "Number of batches 4\n",
      "Epoch is 520\n",
      "Number of batches 4\n",
      "Epoch is 530\n",
      "Number of batches 4\n",
      "Epoch is 540\n",
      "Number of batches 4\n",
      "Epoch is 550\n",
      "Number of batches 4\n",
      "Epoch is 560\n",
      "Number of batches 4\n",
      "Epoch is 570\n",
      "Number of batches 4\n",
      "Epoch is 580\n",
      "Number of batches 4\n",
      "Epoch is 590\n",
      "Number of batches 4\n",
      "Epoch is 600\n",
      "Number of batches 4\n",
      "Epoch is 610\n",
      "Number of batches 4\n",
      "Epoch is 620\n",
      "Number of batches 4\n",
      "Epoch is 630\n",
      "Number of batches 4\n",
      "Epoch is 640\n",
      "Number of batches 4\n",
      "Epoch is 650\n",
      "Number of batches 4\n",
      "Epoch is 660\n",
      "Number of batches 4\n",
      "Epoch is 670\n",
      "Number of batches 4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m     args \u001b[39m=\u001b[39m ARGS()\n\u001b[0;32m     11\u001b[0m     train(args)\n\u001b[1;32m---> 12\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[160], line 11\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m         args\u001b[39m.\u001b[39mn_train \u001b[39m=\u001b[39m \u001b[39m512\u001b[39m \n\u001b[0;32m     10\u001b[0m args \u001b[39m=\u001b[39m ARGS()\n\u001b[1;32m---> 11\u001b[0m train(args)\n",
      "Cell \u001b[1;32mIn[157], line 62\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mint\u001b[39m(X_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m BATCH_SIZE)) :\n\u001b[0;32m     60\u001b[0m     x \u001b[39m=\u001b[39m get_x(X_train, index, BATCH_SIZE)\n\u001b[1;32m---> 62\u001b[0m     d_loss, g_loss \u001b[39m=\u001b[39m gan\u001b[39m.\u001b[39;49mtrain_both(x)\n\u001b[0;32m     64\u001b[0m     d_loss_l\u001b[39m.\u001b[39mappend(d_loss) \n\u001b[0;32m     65\u001b[0m     g_loss_l\u001b[39m.\u001b[39mappend(g_loss)\n",
      "Cell \u001b[1;32mIn[144], line 75\u001b[0m, in \u001b[0;36mGAN.train_both\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m \n\u001b[0;32m     74\u001b[0m \u001b[39m# 가짜로 만든 이미지가 진짜로 인식하도록\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m g_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_on_batch(z, np\u001b[39m.\u001b[39;49marray([\u001b[39m1\u001b[39;49m] \u001b[39m*\u001b[39;49m ln)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscriminator\u001b[39m.\u001b[39mtrainable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \n\u001b[0;32m     78\u001b[0m \u001b[39mreturn\u001b[39;00m d_loss, g_loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def main() :\n",
    "    class ARGS :\n",
    "        def __init__(args) :\n",
    "            args.batch_size = 128\n",
    "            args.epochs = 3000\n",
    "            args.output_fold = 'GAN_OUT_0605_cifar10' \n",
    "            args.input_dim = 30 \n",
    "            args.n_train = 512 \n",
    "        \n",
    "    args = ARGS()\n",
    "    train(args)\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train, y_train), (_, _) = mnist.load_data()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
